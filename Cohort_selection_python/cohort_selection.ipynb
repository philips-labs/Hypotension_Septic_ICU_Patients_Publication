{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125cf384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import keyboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b99c8729",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 17:21:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/06/15 17:21:28 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).\n",
      "22/06/15 17:21:28 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/06/15 17:21:28 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/06/15 17:21:28 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from myspark import load_spark, assert_pyspark\n",
    "spark = load_spark()\n",
    "F, Window, types = assert_pyspark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27199471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://130.140.51.208:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[12]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MyApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f78c45db1d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "baddfc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These file paths include sensitive information that is not suitable for publishing externally.\n",
    "# If internal researchers want to use these files internally, please contact Takahiro Kiritoshi.\n",
    "eicu_path = \n",
    "eicu_hsi_path = \n",
    "project_path_1 = \n",
    "project_path_2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea726a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_eicu_patients_baseline(eicu_path, eicu_hsi_path, disease, sepsis):\n",
    "    \n",
    "    # disease category: 'Medical', 'Noncardiac_Surg', 'Cardiac_Surg', 'Trauma'\n",
    "    # sepsis: if sepsis, 1, else, 0\n",
    "    \n",
    "    min_icu_stay = 1 * 24 * 60 # minutes\n",
    "    max_icu_stay = 14 * 24 * 60 # minutes\n",
    "    min_a_line_duration = 6 * 60 # minutes\n",
    "    min_exposure_duration = 24 * 60 # minutes\n",
    "    invasive_data_missing_percentage = 30\n",
    "    adult_patient_age = 18\n",
    "    lower_BMI, upper_BMI = 10, 60\n",
    "    \n",
    "    patients_df = pd.read_parquet(os.path.join(eicu_path, 'patient.parquet'))\n",
    "    patients_df = patients_df[['patientUnitStayID', 'gender', 'age', 'ethnicity', 'admissionHeight', 'admissionWeight',\n",
    "                               'hospitalAdmitYear', 'hospitalDischargeOffset', 'unitAdmitSource',\n",
    "                               'unitDischargeOffset', 'hospitalID']]\n",
    "    # Overall cohort size \n",
    "    print('Total ICU stays=', patients_df['patientUnitStayID'].nunique())\n",
    "    \n",
    "    # Merge with admission category dataframe\n",
    "    icu_stay_type_df = pd.read_csv(os.path.join(project_path_1, 'Entire_Cohort_ICU_admission_category.csv'))\n",
    "    icu_stay_type_df.rename(columns={'patientunitstayid': 'patientUnitStayID'}, inplace=True)\n",
    "    patients_df = patients_df.merge(icu_stay_type_df.loc[:, ['patientUnitStayID', 'ICU_admission_category']],\n",
    "                                    on='patientUnitStayID', how='left')\n",
    "    \n",
    "    # Merge with sepsis dataframe\n",
    "    infection_sepsis_df = pd.read_parquet(os.path.join(project_path_1, 'Entire_Cohort_Sepsis.parquet'))\n",
    "    patients_df = patients_df.merge(infection_sepsis_df.loc[:, ['patientUnitStayID', 'Sepsis']],\n",
    "                                    on='patientUnitStayID', how='left')\n",
    "    \n",
    "    # Drop patients with invalid age, sex, admission diagnosis\n",
    "    patients_df['age'] = pd.to_numeric(patients_df['age'], errors='coerce')\n",
    "    patients_df.dropna(subset=['age', 'ICU_admission_category'], inplace=True)\n",
    "    target_idx = patients_df['gender'].str.contains('Male|Female')\n",
    "    patients_df = patients_df.loc[target_idx, :]    \n",
    "    print('ICU stays with valid age, sex, admission diagnosis=', len(patients_df))\n",
    "    \n",
    "    # Select target patients\n",
    "    if sepsis==1:\n",
    "        target_idx_disease = np.logical_and(patients_df['ICU_admission_category']==disease, patients_df['Sepsis']==sepsis)\n",
    "        patients_df = patients_df.loc[target_idx_disease, :]\n",
    "        print('Total ICU stays with sepsis=', patients_df['patientUnitStayID'].nunique())\n",
    "    else:\n",
    "        patients_df = patients_df.loc[patients_df['ICU_admission_category']==disease, :]\n",
    "        print(f'Total ICU stays with {disease}=', patients_df['patientUnitStayID'].nunique())\n",
    "    \n",
    "    # Limit patients to those aged 18 or more\n",
    "    target_idx = patients_df['age'] >= adult_patient_age\n",
    "    patients_df = patients_df.loc[target_idx, :]\n",
    "    overall_cohort_size = len(patients_df)\n",
    "    print('ICU stays with age >= 18 years=', overall_cohort_size)\n",
    "    print('    Note: SOFA score is calculated only on patients >= 18 years')\n",
    "    assert (patients_df['age'] >= adult_patient_age).all()\n",
    "    \n",
    "    # Merge with APACHE dataframe\n",
    "    apache_df = pd.read_parquet(os.path.join(eicu_path, 'apachePatientResults.parquet'))\n",
    "    apache_df.rename(columns={'patientunitstayid': 'patientUnitStayID'}, inplace=True)\n",
    "    target_idx_IVa = np.logical_and(apache_df['apacheversion'] == 'IVa', apache_df['apachescore'] > 0)\n",
    "    apache_df = apache_df.loc[target_idx_IVa, ['patientUnitStayID', 'apachescore', 'actualicumortality', 'actualhospitalmortality']]\n",
    "    patients_df = patients_df.merge(apache_df, on='patientUnitStayID', how='inner')\n",
    "    \n",
    "    # As APACHE table has the reliable mortality, we added the time of death at this point\n",
    "    patients_df['time_of_death'] = patients_df.loc[:, ['unitDischargeOffset', 'hospitalDischargeOffset']].min(axis=1)\n",
    "    patients_df.loc[patients_df['actualicumortality'] == 'ALIVE', 'time_of_death'] = np.NaN\n",
    "    \n",
    "    w_apache = len(patients_df)\n",
    "    print('ICU stays w/ APACHE score=', w_apache)\n",
    "    print('  w/o APACHE score=', overall_cohort_size - w_apache)\n",
    "    \n",
    "    # BMI\n",
    "    # Setting admission height and weight to be NaN if zero\n",
    "    patients_df.loc[patients_df['admissionHeight'] == 0, 'admissionHeight'] = np.NaN\n",
    "    patients_df.loc[patients_df['admissionWeight'] == 0, 'admissionWeight'] = np.NaN\n",
    "    # Computing BMI\n",
    "    patients_df['admission_BMI'] = patients_df['admissionWeight'] / ((patients_df['admissionHeight'] * 0.01) ** 2)\n",
    "    # Retaining only BMI > 10 and BMI < 60 of course not null\n",
    "    target_idx = np.logical_and.reduce((patients_df['admission_BMI'].notnull(), patients_df['admission_BMI'] > lower_BMI, patients_df['admission_BMI'] < upper_BMI))\n",
    "    patients_df = patients_df.loc[target_idx, :]\n",
    "    w_BMI = len(patients_df)\n",
    "    print('Patients w/ valid BMI=', w_BMI)\n",
    "    print('  w/o valid BMI=', w_apache - w_BMI)\n",
    "    \n",
    "    # DNR\n",
    "    dnr_df = pd.read_parquet(os.path.join(eicu_hsi_path, 'icustay_details_clean.parquet'))\n",
    "    dnr_df.rename(columns={'patientunitstayid': 'patientUnitStayID'}, inplace=True)\n",
    "    patients_df = patients_df.merge(dnr_df.loc[:, ['patientUnitStayID', 'DNR']], on='patientUnitStayID', how='left')\n",
    "    patients_df = patients_df.loc[patients_df['DNR']==False, :]\n",
    "    no_DNR = len(patients_df)\n",
    "    print('Patients w/o DNR status=', no_DNR)\n",
    "    print('  w/ DNR status=', w_BMI-no_DNR)\n",
    "    \n",
    "    # Mechanical support\n",
    "    resp_charting_device_df = pd.read_parquet(os.path.join(project_path_2, 'device_usage_from_resp_charting.parquet'))\n",
    "    nurse_charting_device_df = pd.read_parquet(os.path.join(project_path_2, 'device_usage_from_nurse_charting.parquet'))\n",
    "    treatments_device_df = pd.read_parquet(os.path.join(project_path_2, 'device_usage_from_treatments.parquet'))\n",
    "    patients_with_devices = pd.concat([treatments_device_df['patientUnitStayID'], nurse_charting_device_df['patientUnitStayID'], resp_charting_device_df['patientUnitStayID']], ignore_index=True)\n",
    "    target_idx = ~patients_df['patientUnitStayID'].isin(patients_with_devices)\n",
    "    patients_df = patients_df.loc[target_idx, :]\n",
    "    no_mechanical_support = len(patients_df)\n",
    "    print('Patients NO MECH. devices=', no_mechanical_support)\n",
    "    print('  with IABP+LVAD+RVAD+BVAD+Impella+ECMO=', no_DNR - no_mechanical_support)\n",
    "    \n",
    "    # LOS<1 or >14 days\n",
    "    target_idx = np.logical_or(patients_df['actualicumortality'] == 'ALIVE', np.logical_and(patients_df['actualicumortality'] == 'EXPIRED', patients_df['time_of_death'] > min_icu_stay))\n",
    "    patients_df = patients_df.loc[target_idx, :]\n",
    "    target_idx = np.logical_and(patients_df['unitDischargeOffset'] >= min_icu_stay, patients_df['unitDischargeOffset'] <= max_icu_stay)\n",
    "    patients_df = patients_df.loc[target_idx, :]\n",
    "    valid_LOS = len(patients_df)\n",
    "    print('Patients stayed in ICU 1-14 days=', valid_LOS)\n",
    "    print('  stayed in ICU <1 day or > 14 days=', no_mechanical_support-valid_LOS)\n",
    "    \n",
    "    # Outcome ascertainment\n",
    "    # Myocardial injury\n",
    "    troponin_df = pd.read_parquet(os.path.join(project_path_2, 'troponin_df_max_value_first_offset.parquet.gzip'))\n",
    "    troponin_df = troponin_df.merge(patients_df.loc[:, ['patientUnitStayID', 'unitDischargeOffset']], how='inner', on='patientUnitStayID')\n",
    "    target_idx = np.logical_and(troponin_df['labResultOffset'] < min_exposure_duration, troponin_df['labResultOffset'] >= 0)\n",
    "    exclude_MI_24h = troponin_df.loc[target_idx, 'patientUnitStayID']\n",
    "    patients_df = patients_df.merge(troponin_df.loc[:, ['patientUnitStayID', 'labResultOffset']], on='patientUnitStayID', how='left')\n",
    "    patients_df.loc[(patients_df['labResultOffset']>patients_df['unitDischargeOffset']) | (patients_df['labResultOffset']<min_exposure_duration), 'labResultOffset'] = np.NaN\n",
    "    patients_df = patients_df.loc[~patients_df['patientUnitStayID'].isin(exclude_MI_24h), :]\n",
    "    no_MI_24h = len(patients_df)\n",
    "    print('Patients w/o MI within 24h=', no_MI_24h)\n",
    "    print('  w/ MI within 24h=', valid_LOS-no_MI_24h)\n",
    "    print('Total patients with Myocardoial injury=', patients_df['labResultOffset'].notnull().sum())\n",
    "    \n",
    "    # AKI\n",
    "    diagnosis_df = pd.read_parquet(os.path.join(eicu_path, 'diagnosis.parquet'))\n",
    "    diagnosis_df = diagnosis_df.merge(patients_df['patientUnitStayID'], on='patientUnitStayID', how='inner')\n",
    "    diagnosis_df['ICD9Code'].fillna('None', inplace=True)\n",
    "    AKI_df = diagnosis_df.loc[diagnosis_df.ICD9Code.str.contains('584.9'), ['patientUnitStayID', 'diagnosisOffset']]\n",
    "    AKI_df = AKI_df.groupby('patientUnitStayID').agg(AKI_offset=('diagnosisOffset', 'min'))\n",
    "    AKI_df.reset_index(inplace=True)\n",
    "    AKI_df = AKI_df.rename(columns = {'index':'patientUnitStayID'})\n",
    "    target_idx = np.logical_and(AKI_df['AKI_offset'] < min_exposure_duration, AKI_df['AKI_offset'] >= 0)\n",
    "    exclude_AKI_24h = AKI_df.loc[target_idx, 'patientUnitStayID']\n",
    "    patients_df = patients_df.merge(AKI_df.loc[:, ['patientUnitStayID', 'AKI_offset']],\n",
    "                                    on='patientUnitStayID', how='left')\n",
    "    \n",
    "    patients_df = patients_df.loc[~patients_df['patientUnitStayID'].isin(exclude_AKI_24h), :]\n",
    "    patients_df.loc[(patients_df['AKI_offset']>patients_df['unitDischargeOffset']) | (patients_df['AKI_offset']<min_exposure_duration), 'AKI_offset'] = np.NaN\n",
    "    no_AKI_24h = len(patients_df)\n",
    "    print('Patients w/o AKI within 24h=', no_AKI_24h)\n",
    "    print('  w/ AKI within 24h=', no_MI_24h-no_AKI_24h)\n",
    "    print('Total patients with AKI=', patients_df['AKI_offset'].notnull().sum())\n",
    "    \n",
    "    del icu_stay_type_df, infection_sepsis_df, apache_df, dnr_df, resp_charting_device_df, nurse_charting_device_df, treatments_device_df, patients_with_devices, troponin_df, diagnosis_df\n",
    "\n",
    "    patients_df.rename(columns={'min_observationOffset': 'first_MBP_timestamp', 'max_observationOffset': 'last_MBP_timestamp',\n",
    "                                'labResultOffset': 'MI_offset', 'time_of_death': 'Death_offset',\n",
    "                                'unitDischargeOffset': 'Discharge_offset'},\n",
    "                       inplace=True)\n",
    "\n",
    "    # Patients who died their discharge time is set to NaN\n",
    "    target_idx = patients_df['Death_offset'].notnull()\n",
    "    patients_df.loc[target_idx, 'Discharge_offset'] = np.NaN\n",
    "    assert len(np.intersect1d(np.where(patients_df['Death_offset'].notnull())[0], np.where(patients_df['Discharge_offset'].notnull())[0])) == 0\n",
    "\n",
    "    events_of_interest = ['MI_offset', 'AKI_offset', 'Death_offset', 'Discharge_offset']\n",
    "    patients_df['patient_last_timestamp'] = patients_df[events_of_interest].max(axis=1)\n",
    "    \n",
    "    return patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "494de27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total ICU stays= 3336449\n",
      "ICU stays with valid age, sex, admission diagnosis= 2412019\n",
      "Total ICU stays with sepsis= 301447\n",
      "ICU stays with age >= 18 years= 301447\n",
      "    Note: SOFA score is calculated only on patients >= 18 years\n",
      "ICU stays w/ APACHE score= 228415\n",
      "  w/o APACHE score= 73032\n",
      "Patients w/ valid BMI= 215653\n",
      "  w/o valid BMI= 12762\n",
      "Patients w/o DNR status= 212516\n",
      "  w/ DNR status= 3137\n",
      "Patients NO MECH. devices= 208367\n",
      "  with IABP+LVAD+RVAD+BVAD+Impella+ECMO= 4149\n",
      "Patients stayed in ICU 1-14 days= 197462\n",
      "  stayed in ICU <1 day or > 14 days= 10905\n",
      "Patients w/o MI within 24h= 168396\n",
      "  w/ MI within 24h= 29066\n",
      "Total patients with Myocardoial injury= 4005\n",
      "Patients w/o AKI within 24h= 137197\n",
      "  w/ AKI within 24h= 31199\n",
      "Total patients with AKI= 4921\n"
     ]
    }
   ],
   "source": [
    "patients_df = filter_eicu_patients_baseline(eicu_path, eicu_hsi_path, 'Medical', 1)\n",
    "patients_df.to_csv(os.path.join(project_path_2, 'sepsis_patients_filters_first_pass.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54f05b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/Taka/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:59: FutureWarning: Index.ravel returning ndarray is deprecated; in a future version this will return a view on self.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total identified patients= 137197\n",
      "Patients with invasive+non-invasive (all 3 BPs per data source must be present) between time of admission and time of death|discharge= 128717\n",
      "Max gap <= 120 minutes= 52261\n"
     ]
    }
   ],
   "source": [
    "def load_preprocess_merged_df(start_p, end_p, merged_cols, patients_df):\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    merged_df_filename = os.path.join(project_path_2, 'invasive_BP_selectively_merged_batch_%s_%s.parquet' % (start_p, end_p))\n",
    "\n",
    "    if os.path.exists(merged_df_filename):\n",
    "        merged_df = pd.read_parquet(merged_df_filename)\n",
    "\n",
    "        if len(merged_df) > 0:\n",
    "            target_idx = merged_df['mergedSystolic'] < merged_df['mergedDiastolic'] + 5\n",
    "            merged_df.loc[target_idx, 'mergedSystolic'] = np.NaN\n",
    "\n",
    "            target_idx = np.logical_or(merged_df['mergedMean'] >= merged_df['mergedSystolic'], merged_df['mergedMean'] <= merged_df['mergedDiastolic'])\n",
    "            merged_df.loc[target_idx, 'mergedMean'] = np.NaN\n",
    "\n",
    "            mbp_nan_idx = merged_df['mergedMean'].isnull()\n",
    "            merged_df.loc[mbp_nan_idx, 'mergedMean'] = merged_df.loc[mbp_nan_idx, 'mergedDiastolic'] +\\\n",
    "                                    (1/3 * (merged_df.loc[mbp_nan_idx, 'mergedSystolic'] - merged_df.loc[mbp_nan_idx, 'mergedDiastolic']))\n",
    "\n",
    "            merged_df['pulse_pressure'] = merged_df['mergedSystolic'] - merged_df['mergedDiastolic']\n",
    "            assert (merged_df.loc[merged_df['pulse_pressure'].notnull(), 'pulse_pressure'] > 0).all()\n",
    "\n",
    "            # Dropping rows that have one or more NaN's in the merged columns\n",
    "            merged_df = merged_df.loc[~merged_df[merged_cols].isnull().any(axis=1), :]\n",
    "            merged_df = merged_df.merge(patients_df[['patientUnitStayID', 'patient_last_timestamp']], on='patientUnitStayID', how='left')\n",
    "            merged_df = merged_df.loc[np.logical_and(merged_df['observationOffset'] > 0, merged_df['observationOffset'] <= merged_df['patient_last_timestamp']), :]\n",
    "\n",
    "            # Sorting by patients and by observation offset\n",
    "            merged_df = merged_df.sort_values(by=['patientUnitStayID', 'observationOffset'])\n",
    "            merged_df.reset_index(drop=True, inplace=True)\n",
    "            merged_df['time_diff'] = merged_df.groupby('patientUnitStayID')['observationOffset'].diff()\n",
    "            assert merged_df['time_diff'].isnull().sum() == merged_df['patientUnitStayID'].nunique()\n",
    "\n",
    "    return (merged_df)\n",
    "\n",
    "######################################################################################################\n",
    "def get_min_max_start_end_idx():\n",
    "\n",
    "    patient_id_bins = np.arange(0, 4000001, 100000)\n",
    "    merged_cols = ['mergedSystolic', 'mergedDiastolic', 'mergedMean', 'pulse_pressure']\n",
    "    patients_df = pd.read_csv(os.path.join(project_path_2, 'sepsis_patients_filters_first_pass.csv'), index_col=0)\n",
    "\n",
    "    f = {'observationOffset': ['max', 'min', 'count'],\n",
    "        'mergedSystolic': ['idxmin', 'min'],\n",
    "        'mergedDiastolic': ['idxmin', 'min'],\n",
    "        'mergedMean': ['idxmin', 'min'],\n",
    "        'pulse_pressure': ['idxmin', 'min'],\n",
    "        'time_diff': ['max'],\n",
    "        'systemicMean': ['count', lambda x: x.isnull().sum()],\n",
    "        'nonInvasiveMean': ['count']\n",
    "        }\n",
    "\n",
    "    gather_df = pd.DataFrame()\n",
    "    for p_idx, p in enumerate(np.arange(1, len(patient_id_bins))):\n",
    "        merged_df = load_preprocess_merged_df(patient_id_bins[p-1], patient_id_bins[p], merged_cols, patients_df)\n",
    "        if len(merged_df) > 0:\n",
    "            temp_df = merged_df[['patientUnitStayID', 'observationOffset', 'time_diff', 'nonInvasiveMean', 'systemicMean']+merged_cols].groupby('patientUnitStayID').agg(f)\n",
    "            temp_df.columns = [\"_\".join(x) for x in temp_df.columns.ravel()]\n",
    "            temp_df.reset_index(inplace=True)\n",
    "            temp_df.rename(columns={'systemicMean_<lambda_0>': 'invasive_MBP_nan_counts'}, inplace=True)\n",
    "            temp_df['BP_contribution'] = -1\n",
    "            temp_df.loc[np.logical_and(temp_df['systemicMean_count'] > 0, temp_df['nonInvasiveMean_count'] > 0), 'BP_contribution'] = 1\n",
    "            temp_df.loc[np.logical_and(temp_df['systemicMean_count'] > 0, temp_df['nonInvasiveMean_count'] == 0), 'BP_contribution'] = 2\n",
    "            temp_df.loc[np.logical_and(temp_df['systemicMean_count'] == 0, temp_df['nonInvasiveMean_count'] > 0), 'BP_contribution'] = 3\n",
    "\n",
    "            for i in temp_df.columns:\n",
    "                if '_idxmin' in i:\n",
    "                    temp_df.loc[:, i.split('_')[0]+'_min_observationOffset'] = merged_df.loc[temp_df[i], 'observationOffset'].values\n",
    "\n",
    "            assert temp_df['patientUnitStayID'].nunique() == len(temp_df)\n",
    "            gather_df = pd.concat([gather_df, temp_df], ignore_index=True)\n",
    "            assert gather_df['patientUnitStayID'].nunique() == len(gather_df)\n",
    "\n",
    "    gather_df.to_csv(os.path.join(project_path_2, 'sepsis_min_max_idx_start_end.csv'))\n",
    "\n",
    "######################################################################################################\n",
    "def filter_eicu_patients_second_set():\n",
    "\n",
    "    eICU_sampling_interval = 5.0\n",
    "    bp_readings_max_gap_allowed = 2 * 60\n",
    "    events_of_interest = ['MI_offset', 'AKI_offset', 'Death_offset', 'Discharge_offset']\n",
    "\n",
    "    patients_df = pd.read_csv(os.path.join(project_path_2, 'sepsis_patients_filters_first_pass.csv'), index_col=0)\n",
    "    print('Total identified patients=', len(patients_df))\n",
    "\n",
    "    gather_df = pd.read_csv(os.path.join(project_path_2, 'sepsis_min_max_idx_start_end.csv'), index_col=0)\n",
    "    print('Patients with invasive+non-invasive (all 3 BPs per data source must be present) between time of admission and time of death|discharge=', len(gather_df))\n",
    "\n",
    "    patients_df = patients_df.merge(gather_df, on=['patientUnitStayID'], how='inner')\n",
    "\n",
    "    # Finding patients that have a gap at the beginning, in between BP readings and at the end of the ICU stay\n",
    "    # Negating this logical or to get valid patients\n",
    "    target_idx = ~np.logical_or.reduce((patients_df['observationOffset_min'] > bp_readings_max_gap_allowed,\\\n",
    "                        patients_df['time_diff_max'] > bp_readings_max_gap_allowed,\\\n",
    "                        (patients_df['patient_last_timestamp'] - patients_df['observationOffset_max']) > bp_readings_max_gap_allowed))\n",
    "\n",
    "    print('Max gap <= %s minutes=' % (bp_readings_max_gap_allowed), target_idx.sum())\n",
    "    patients_df = patients_df.loc[target_idx, :]\n",
    "\n",
    "    # Few patients (76 to be precise) that have AKI offset after death|discharge\n",
    "    # We adjust the time of AKI to be the time of death|discharge since ICD 9 code recorded time is unreliable\n",
    "    for m in ['MI_offset', 'AKI_offset']:\n",
    "        for d in ['Death_offset', 'Discharge_offset']:\n",
    "            target_idx = patients_df[m] > patients_df[d]\n",
    "            if (target_idx).sum() > 0:\n",
    "                patients_df.loc[target_idx, m] = patients_df.loc[target_idx, d]\n",
    "            assert (patients_df[m] > patients_df[d]).sum() == 0\n",
    "\n",
    "    patients_df.to_csv(os.path.join(project_path_2, 'sepsis_patients_filters_second_pass.csv'))\n",
    "\n",
    "######################################################################################################\n",
    "def find_minimum_for_x_hours():\n",
    "\n",
    "    below_max_window = 2 * 60 # 2 hours\n",
    "    patient_id_bins = np.arange(0, 4000001, 100000)\n",
    "    merged_cols = ['mergedSystolic', 'mergedDiastolic', 'mergedMean', 'pulse_pressure']\n",
    "    events_of_interest = ['MI', 'AKI', 'Death', 'Discharge']\n",
    "\n",
    "    patients_df_second_pass = pd.read_csv(os.path.join(project_path_2, 'sepsis_patients_filters_second_pass.csv'), index_col=0)\n",
    "\n",
    "    below_cols = ['below_%s_%s_%s' % (below_max_window, j, e) for j in merged_cols for e in events_of_interest]\n",
    "    patients_df_second_pass[below_cols] = np.NaN\n",
    "\n",
    "    for p_idx, p in enumerate(np.arange(1, len(patient_id_bins))):\n",
    "        merged_df = load_preprocess_merged_df(patient_id_bins[p-1], patient_id_bins[p], merged_cols, patients_df_second_pass)\n",
    "\n",
    "        if len(merged_df) > 0:\n",
    "            unique_patients = list(merged_df.loc[merged_df['patientUnitStayID'].isin(patients_df_second_pass['patientUnitStayID']), 'patientUnitStayID'].unique())\n",
    "            for u in unique_patients:\n",
    "                for e in events_of_interest:\n",
    "                    event_end = patients_df_second_pass.loc[patients_df_second_pass['patientUnitStayID'] == u, '%s_offset' % (e)].values[0]\n",
    "                    if ~np.isnan(event_end):\n",
    "                        patient_data = merged_df.loc[np.logical_and(merged_df['patientUnitStayID'] == u, merged_df['observationOffset'] <= event_end), :]\n",
    "\n",
    "                        if len(patient_data) > 1:\n",
    "                            patient_start = patient_data['observationOffset'].values[0]\n",
    "                            patient_end = patient_data['observationOffset'].values[-1]\n",
    "\n",
    "                            for m in merged_cols:\n",
    "                                interpolated_signal = np.interp(np.arange(patient_start, patient_end, 1), patient_data['observationOffset'], patient_data[m])\n",
    "\n",
    "                                # Few patients have constant BP readings i.e. flat line. I am increasing the max by 1 to execute the for loop below\t\n",
    "                                min_interpolated_signal = np.min(interpolated_signal)\n",
    "                                max_interpolated_signal = np.max(interpolated_signal)\n",
    "                                if min_interpolated_signal == max_interpolated_signal:\n",
    "                                    max_interpolated_signal += 1\n",
    "\n",
    "                                for i in np.arange(min_interpolated_signal, max_interpolated_signal, 1):\n",
    "                                    if np.sum(interpolated_signal <= i) >= below_max_window:\n",
    "                                        patients_df_second_pass.loc[patients_df_second_pass['patientUnitStayID'] == u,\\\n",
    "                                                        'below_%s_%s_%s' % (below_max_window, m, e)] = i\n",
    "                                        break\n",
    "\n",
    "    patients_df_second_pass.to_csv(os.path.join(project_path_2, 'sepsis_patients_filters_second_pass.csv'))\n",
    "\n",
    "######################################################################################################\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # like mimic for eICU we have subject_id=patientHealthSystemStayID and icustay_id=patientUnitStayID\n",
    "    eicu_path = \n",
    "    eicu_hsi_path = \n",
    "\n",
    "\n",
    "    get_min_max_start_end_idx()\n",
    "\n",
    "    filter_eicu_patients_second_set()\n",
    "\n",
    "    find_minimum_for_x_hours()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0da5245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_third_pass_filters(eicu_path):\n",
    "\n",
    "    # settings\n",
    "    past_history_to_exclude = ['Exclude_PastHistory_Chronic_Kidney_Disease', 'Exclude_PastHistory_Myocardial_Infarction',\\\n",
    "                    'Exclude_PastHistory_Stroke', 'Exclude_PastHistory_Coronary_Artery_Disease']\n",
    "    \n",
    "    # Loading second pass patient filter\n",
    "    patients_df = pd.read_csv(os.path.join(project_path_2, 'sepsis_patients_filters_second_pass.csv'), index_col=0)\n",
    "    no_gap_2h = len(patients_df)\n",
    "    print('Patients w/o >2 hours BP reading gap=', no_gap_2h)\n",
    "\n",
    "    # Loading df with hospital information ~459 hospitals in total\n",
    "    hospital_df = pd.read_parquet(os.path.join(eicu_path, 'hospital.parquet'))\n",
    "    hospital_df['hospitalID'] = hospital_df['hospitalID'].astype(int)\n",
    "    patients_df = patients_df.merge(hospital_df, on='hospitalID', how='left')\n",
    "\n",
    "    # Medication\n",
    "    admission_drug_df = pd.read_parquet(os.path.join(project_path_2, 'all_admission_drug.parquet'))\n",
    "    admission_drug_df['patientUnitStayID'] = admission_drug_df['patientUnitStayID'].astype(int)\n",
    "    patients_df = patients_df.merge(admission_drug_df, on='patientUnitStayID', how='left')\n",
    "    patients_df = patients_df.fillna({'aspirin': 0, 'diuretics': 0, 'ace_inhibitors': 0, 'ARBs': 0, 'beta_blockers': 0, 'ca_channel_blockers': 0})\n",
    "\n",
    "    # Past History\n",
    "    previous_history_df = pd.read_parquet(os.path.join(project_path_2, 'all_previous_history.parquet'))\n",
    "    previous_history_df['patientUnitStayID'] = previous_history_df['patientUnitStayID'].astype(int)\n",
    "    patients_df = patients_df.merge(previous_history_df, on='patientUnitStayID', how='left')\n",
    "\n",
    "    patients_df = patients_df.loc[patients_df[past_history_to_exclude].sum(axis=1) == 0, :]\n",
    "    w_o_past_history = len(patients_df)\n",
    "    print('Patients who have no past medical history=', w_o_past_history)\n",
    "    print('  who have past medical history=', no_gap_2h-w_o_past_history)\n",
    "\n",
    "    # Lab\n",
    "    df_lab = pd.read_parquet(os.path.join(project_path_2, 'initial_lab_values.parquet'))\n",
    "    patients_df = patients_df.merge(df_lab.loc[:, ['patientUnitStayID', 'Initial_Hgb', 'Initial_albumin',\n",
    "                                                   'Initial_WBCx1000', 'Initial_BUN', 'Initial_lactate', 'eGFR']],\n",
    "                                    on='patientUnitStayID', how='left')\n",
    "    patients_df = patients_df.loc[patients_df['eGFR']>=60, :]\n",
    "    patients_df = patients_df.drop(['eGFR'], axis=1)\n",
    "    eGFR_60 = len(patients_df)\n",
    "    print('Patients w/ eGFR >= 60=', eGFR_60)\n",
    "    print('  w/ eGFR < 60=', w_o_past_history-eGFR_60)\n",
    "              \n",
    "    patients_df.to_csv(os.path.join(project_path_2, 'sepsis_patients_filters_third_pass.csv'))\n",
    "    \n",
    "    return patients_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8932adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patients w/o >2 hours BP reading gap= 52261\n",
      "Patients who have no past medical history= 34012\n",
      "  who have past medical history= 18249\n",
      "Patients w/ eGFR >= 60= 22494\n",
      "  w/ eGFR < 60= 11518\n"
     ]
    }
   ],
   "source": [
    "patients_df = perform_third_pass_filters(eicu_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b1d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sepsis = spark.read.csv(os.path.join(project_path_2, 'sepsis_patients_filters_third_pass.csv'),\n",
    "                    inferSchema = True, header = True)\n",
    "\n",
    "df_event = spark.read.parquet('/home/lx099-scratch/users/asif/multitask_events.parquet',)\n",
    "\n",
    "df_vent = (df_event.filter(F.col('Event')=='InvasiveVentilation')\n",
    "           .filter((F.col('StartOffset')<24*60)&(F.col('EndOffset')>0))\n",
    "           .withColumn('On_vent', F.lit(1).cast('int'))\n",
    "           .groupBy('patientUnitStayID')\n",
    "           .agg(F.max('On_vent').alias('On_vent')))\n",
    "\n",
    "df_sepsis = (df_sepsis.join(df_vent, on='patientUnitStayID', how='left')\n",
    "             .na.fill({'On_vent': 0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e4a5ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_outcome(df, outcome):\n",
    "    df_outcome = (df.select('patientUnitStayID', F.col('age').cast('int').alias('age'),\n",
    "                            F.when(F.col('gender')=='Female', 0)\n",
    "                            .otherwise(1).alias('male'),\n",
    "                            F.when(F.col('ethnicity').isNull(), 'Other/Unknown')\n",
    "                            .otherwise(F.col('ethnicity')).alias('ethnicity'),\n",
    "                            F.round(F.col('admission_BMI'), 1).alias('BMI'),\n",
    "                            'apachescore',\n",
    "                            F.when(F.col('unitAdmitSource')=='Emergency Department', 'Emergency_Department')\n",
    "                            .when(F.col('unitAdmitSource')=='Other Hospital', 'Other_Hospital')\n",
    "                            .when((F.col('unitAdmitSource').isNull())|(F.col('unitAdmitSource')=='Other'), 'Other/Unknown')\n",
    "                            .when(F.col('unitAdmitSource')=='Direct Admit', 'Elective')\n",
    "                            .otherwise('Other_Ward').alias('Admission_Type'),\n",
    "                            F.when(F.col('hospitalAdmitYear')<=2008, '2004-2008')\n",
    "                            .when(F.col('hospitalAdmitYear')>=2013, '2013-2016')\n",
    "                            .otherwise('2009-2012').alias('Admit_Year'),\n",
    "                            F.when(F.col('numbedscategory').isNull(), 'Unknown')\n",
    "                            .otherwise(F.col('numbedscategory')).alias('Hospital_Bed_Size'),\n",
    "                            'aspirin', 'diuretics', 'ace_inhibitors', 'ARBs', 'beta_blockers', 'ca_channel_blockers',\n",
    "                            F.when(F.col('Initial_Hgb').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_Hgb')<8, '<8')\n",
    "                            .when(F.col('Initial_Hgb')>=11, '>=11')\n",
    "                            .otherwise('8<=Hb<11').alias('Hb'),\n",
    "                            F.when(F.col('Initial_albumin').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_albumin')<2, '<2')\n",
    "                            .when(F.col('Initial_albumin')>=3, '>=3')\n",
    "                            .otherwise('2<=Alb<3').alias('Alb'),\n",
    "                            F.when(F.col('Initial_WBCx1000').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_WBCx1000')<4, '<4')\n",
    "                            .when(F.col('Initial_WBCx1000')>=12, '>=12')\n",
    "                            .otherwise('4<=WBCx1000<12').alias('WBCx1000'),\n",
    "                            F.when(F.col('Initial_BUN').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_BUN')<=30, '<=30')\n",
    "                            .otherwise('>30').alias('BUN'),\n",
    "                            F.when(F.col('Initial_lactate').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_lactate')<2, '<2')\n",
    "                            .when(F.col('Initial_lactate')>=5, '>=5')\n",
    "                            .otherwise('2<=Lac<5').alias('Lac'),\n",
    "                            'PastHistory_Hypertension', 'PastHistory_Diabetes', 'PastHistory_COPD',\n",
    "                            'PastHistory_Congestive_Heart_Failure', 'PastHistory_Peripheral_Vascular_Disease',\n",
    "                            'PastHistory_Valve_disease', 'PastHistory_Pulmonary_Embolism',\n",
    "                            'PastHistory_Neuromuscular_Disease', 'PastHistory_Hypothyroidism',\n",
    "                            'PastHistory_Liver_Disease', 'PastHistory_AIDS', 'PastHistory_Cancer_Tumor',\n",
    "                            'PastHistory_Arthritis_Vasculitis', 'PastHistory_Coagulopathy',\n",
    "                            'PastHistory_Anemia', 'PastHistory_Home_Oxygen', 'PastHistory_Organ_Transplant',\n",
    "                            'On_vent',\n",
    "                            F.when(F.col('below_120_mergedSystolic_'+outcome).isNotNull(), F.col('below_120_mergedSystolic_'+outcome))\n",
    "                            .when(F.col('below_120_mergedSystolic_Death').isNotNull(), F.col('below_120_mergedSystolic_Death'))\n",
    "                            .otherwise(F.col('below_120_mergedSystolic_Discharge')).alias('Systolic_below_120'),\n",
    "                            F.when(F.col('below_120_mergedDiastolic_'+outcome).isNotNull(), F.col('below_120_mergedDiastolic_'+outcome))\n",
    "                            .when(F.col('below_120_mergedDiastolic_Death').isNotNull(), F.col('below_120_mergedDiastolic_Death'))\n",
    "                            .otherwise(F.col('below_120_mergedDiastolic_Discharge')).alias('Diastolic_below_120'),\n",
    "                            F.when(F.col('below_120_mergedMean_'+outcome).isNotNull(), F.col('below_120_mergedMean_'+outcome))\n",
    "                            .when(F.col('below_120_mergedMean_Death').isNotNull(), F.col('below_120_mergedMean_Death'))\n",
    "                            .otherwise(F.col('below_120_mergedMean_Discharge')).alias('Mean_below_120'),\n",
    "                            F.when(F.col('below_120_pulse_pressure_'+outcome).isNotNull(), F.col('below_120_pulse_pressure_'+outcome))\n",
    "                            .when(F.col('below_120_pulse_pressure_Death').isNotNull(), F.col('below_120_pulse_pressure_Death'))\n",
    "                            .otherwise(F.col('below_120_pulse_pressure_Discharge')).alias('Pulsepressure_below_120'),\n",
    "                            F.when(F.col('actualicumortality')=='ALIVE', 0)\n",
    "                            .otherwise(1).alias('ICU_mortality'),\n",
    "                            F.when(F.col('actualhospitalmortality')=='ALIVE', 0)\n",
    "                            .otherwise(1).alias('Hospital_mortality'),\n",
    "                            F.when(F.col(outcome+'_offset').isNotNull(), 1)\n",
    "                            .otherwise(0).alias(outcome),\n",
    "                            F.when(F.col(outcome+'_offset').isNotNull(), F.col(outcome+'_offset'))\n",
    "                            .when(F.col('Death_offset').isNotNull(), F.col('Death_offset'))\n",
    "                            .otherwise(F.col('Discharge_offset')).alias('Outcome_offset')))\n",
    "    \n",
    "    return df_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10df051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Death = get_df_outcome(df_sepsis, 'Death')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "629d25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_vsif = spark.read.parquet(os.path.join(project_path_2, 'vasopressors.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d44b521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_outcome_vsif(df_outcome, df_vsif):\n",
    "    w_lag = (Window\n",
    "             .partitionBy('patientUnitStayID', 'drugConcept')\n",
    "             .orderBy(F.col('infusionOffset').desc())\n",
    "            )\n",
    "\n",
    "    vsif_amount = (df_outcome.select('patientUnitStayID', 'Outcome_offset')\n",
    "                   .join(df_vsif, 'patientUnitStayID')\n",
    "                   .withColumn('infusionEndOffset', F.lag(F.col('infusionOffset')).over(w_lag))\n",
    "                   .filter((F.col('infusionEndOffset')>0)&(F.col('infusionOffset')<F.col('Outcome_offset')))\n",
    "                   .withColumn('Start',\n",
    "                               F.when(F.col('infusionOffset')>=0, F.col('infusionOffset'))\n",
    "                               .otherwise(0).cast('int'))\n",
    "                   .withColumn('End',\n",
    "                               F.when(F.col('infusionEndOffset')>=F.col('Outcome_offset'), F.col('Outcome_offset'))\n",
    "                               .when((F.col('infusionEndOffset').isNull())&(F.col('Start')+60<=F.col('Outcome_offset')), 60)\n",
    "                               .when((F.col('infusionEndOffset').isNull())&(F.col('Start')+60>F.col('Outcome_offset')), F.col('Outcome_offset'))\n",
    "                               .otherwise(F.col('infusionEndOffset')).cast('int'))\n",
    "                   .orderBy('patientUnitStayID', 'drugConcept', 'Start')\n",
    "                   .withColumn('Duration', (F.col('End')-F.col('Start')).cast('int'))\n",
    "                   .withColumn('Amount', F.col('NEEquivalentDrugRate')*F.col('Duration'))\n",
    "                   .groupBy('patientUnitStayID')\n",
    "                   .agg(F.round(F.sum('Amount'), 2).alias('TotalAmountNEE')))\n",
    "    \n",
    "    df_outcome_vsif = (df_outcome.join(vsif_amount, 'patientUnitStayID', 'left')\n",
    "                       .withColumn('NEE_mcg_perkg_perminx1000', F.round(1000*F.col('TotalAmountNEE')/F.col('Outcome_offset')).cast('int'))\n",
    "                       .fillna(0, subset=['TotalAmountNEE', 'NEE_mcg_perkg_perminx1000']))\n",
    "    \n",
    "    return df_outcome_vsif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12a4e702",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/06/15 17:39:37 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_Death_vsif = get_df_outcome_vsif(df_Death, df_vsif).toPandas()\n",
    "df_Death_vsif.to_csv(os.path.join(project_path_2, 'Sepsis_Death.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ebabb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_outcome_composite(df, outcome):\n",
    "    df_outcome = (df.select('patientUnitStayID', F.col('age').cast('int').alias('age'),\n",
    "                            F.when(F.col('gender')=='Female', 0)\n",
    "                            .otherwise(1).alias('male'),\n",
    "                            F.when(F.col('ethnicity').isNull(), 'Other/Unknown')\n",
    "                            .otherwise(F.col('ethnicity')).alias('ethnicity'),\n",
    "                            F.round(F.col('admission_BMI'), 1).alias('BMI'),\n",
    "                            'apachescore',\n",
    "                            F.when(F.col('unitAdmitSource')=='Emergency Department', 'Emergency_Department')\n",
    "                            .when(F.col('unitAdmitSource')=='Other Hospital', 'Other_Hospital')\n",
    "                            .when((F.col('unitAdmitSource').isNull())|(F.col('unitAdmitSource')=='Other'), 'Other/Unknown')\n",
    "                            .when(F.col('unitAdmitSource')=='Direct Admit', 'Elective')\n",
    "                            .otherwise('Other_Ward').alias('Admission_Type'),\n",
    "                            F.when(F.col('hospitalAdmitYear')<=2008, '2004-2008')\n",
    "                            .when(F.col('hospitalAdmitYear')>=2013, '2013-2016')\n",
    "                            .otherwise('2009-2012').alias('Admit_Year'),\n",
    "                            F.when(F.col('numbedscategory').isNull(), 'Unknown')\n",
    "                            .otherwise(F.col('numbedscategory')).alias('Hospital_Bed_Size'),\n",
    "                            'aspirin', 'diuretics', 'ace_inhibitors', 'ARBs', 'beta_blockers', 'ca_channel_blockers',\n",
    "                            F.when(F.col('Initial_Hgb').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_Hgb')<8, '<8')\n",
    "                            .when(F.col('Initial_Hgb')>=11, '>=11')\n",
    "                            .otherwise('8<=Hb<11').alias('Hb'),\n",
    "                            F.when(F.col('Initial_albumin').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_albumin')<2, '<2')\n",
    "                            .when(F.col('Initial_albumin')>=3, '>=3')\n",
    "                            .otherwise('2<=Alb<3').alias('Alb'),\n",
    "                            F.when(F.col('Initial_WBCx1000').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_WBCx1000')<4, '<4')\n",
    "                            .when(F.col('Initial_WBCx1000')>=12, '>=12')\n",
    "                            .otherwise('4<=WBCx1000<12').alias('WBCx1000'),\n",
    "                            F.when(F.col('Initial_BUN').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_BUN')<=30, '<=30')\n",
    "                            .otherwise('>30').alias('BUN'),\n",
    "                            F.when(F.col('Initial_lactate').isNull(), 'No_reading_available')\n",
    "                            .when(F.col('Initial_lactate')<2, '<2')\n",
    "                            .when(F.col('Initial_lactate')>=5, '>=5')\n",
    "                            .otherwise('2<=Lac<5').alias('Lac'),\n",
    "                            'PastHistory_Hypertension', 'PastHistory_Diabetes', 'PastHistory_COPD',\n",
    "                            'PastHistory_Congestive_Heart_Failure', 'PastHistory_Peripheral_Vascular_Disease',\n",
    "                            'PastHistory_Valve_disease', 'PastHistory_Pulmonary_Embolism',\n",
    "                            'PastHistory_Neuromuscular_Disease', 'PastHistory_Hypothyroidism',\n",
    "                            'PastHistory_Liver_Disease', 'PastHistory_AIDS', 'PastHistory_Cancer_Tumor',\n",
    "                            'PastHistory_Arthritis_Vasculitis', 'PastHistory_Coagulopathy',\n",
    "                            'PastHistory_Anemia', 'PastHistory_Home_Oxygen', 'PastHistory_Organ_Transplant',\n",
    "                            'On_vent',\n",
    "                            F.when(F.col('below_120_mergedSystolic_'+outcome).isNotNull(), F.col('below_120_mergedSystolic_'+outcome))\n",
    "                            .when(F.col('below_120_mergedSystolic_Death').isNotNull(), F.col('below_120_mergedSystolic_Death'))\n",
    "                            .otherwise(F.col('below_120_mergedSystolic_Discharge')).alias('Systolic_below_120'),\n",
    "                            F.when(F.col('below_120_mergedDiastolic_'+outcome).isNotNull(), F.col('below_120_mergedDiastolic_'+outcome))\n",
    "                            .when(F.col('below_120_mergedDiastolic_Death').isNotNull(), F.col('below_120_mergedDiastolic_Death'))\n",
    "                            .otherwise(F.col('below_120_mergedDiastolic_Discharge')).alias('Diastolic_below_120'),\n",
    "                            F.when(F.col('below_120_mergedMean_'+outcome).isNotNull(), F.col('below_120_mergedMean_'+outcome))\n",
    "                            .when(F.col('below_120_mergedMean_Death').isNotNull(), F.col('below_120_mergedMean_Death'))\n",
    "                            .otherwise(F.col('below_120_mergedMean_Discharge')).alias('Mean_below_120'),\n",
    "                            F.when(F.col('below_120_pulse_pressure_'+outcome).isNotNull(), F.col('below_120_pulse_pressure_'+outcome))\n",
    "                            .when(F.col('below_120_pulse_pressure_Death').isNotNull(), F.col('below_120_pulse_pressure_Death'))\n",
    "                            .otherwise(F.col('below_120_pulse_pressure_Discharge')).alias('Pulsepressure_below_120'),\n",
    "                            F.when(F.col('actualicumortality')=='ALIVE', 0)\n",
    "                            .otherwise(1).alias('ICU_mortality'),\n",
    "                            F.when(F.col('actualhospitalmortality')=='ALIVE', 0)\n",
    "                            .otherwise(1).alias('Hospital_mortality'),\n",
    "                            F.when(F.col(outcome+'_offset').isNotNull(), 1)\n",
    "                            .otherwise(0).alias(outcome),\n",
    "                            F.when(F.col(outcome+'_offset').isNotNull(), 1)\n",
    "                            .when(F.col('Death_offset').isNotNull(), 1)\n",
    "                            .otherwise(0).alias(outcome+'_composite'),\n",
    "                            F.when(F.col('Death_offset').isNotNull(), F.col('Death_offset'))\n",
    "                            .when(F.col(outcome+'_offset').isNotNull(), F.col(outcome+'_offset'))\n",
    "                            .otherwise(F.col('Discharge_offset')).alias('Outcome_offset')))\n",
    "    \n",
    "    return df_outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5062abe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_AKI_composite = get_df_outcome_composite(df_sepsis, 'AKI')\n",
    "df_MI_composite = get_df_outcome_composite(df_sepsis, 'MI')\n",
    "df_AKI_vsif_composite = get_df_outcome_vsif(df_AKI_composite, df_vsif).toPandas()\n",
    "df_MI_vsif_composite = get_df_outcome_vsif(df_MI_composite, df_vsif).toPandas()\n",
    "df_AKI_vsif_composite.to_csv(os.path.join(project_path_2, 'Sepsis_AKI_composite.csv'), index=False)\n",
    "df_MI_vsif_composite.to_csv(os.path.join(project_path_2, 'Sepsis_MI_composite.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77926ecd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
